{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n5KIVZtsqeVY",
    "outputId": "8e6d5a61-e7b8-4cbe-d4f8-c42b34041767"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (4.0.0)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.9.0)\n",
      "Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.16.0)\n",
      "Collecting trl\n",
      "  Downloading trl-0.20.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.46.1-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.54.1)\n",
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.11.0.post1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.34.3)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.6.0+cu124)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.5.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.4)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.15)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.14.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.7.14)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
      "Downloading trl-0.20.0-py3-none-any.whl (504 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m504.6/504.6 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading bitsandbytes-0.46.1-py3-none-manylinux_2_24_x86_64.whl (72.9 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72.9/72.9 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading faiss_cpu-1.11.0.post1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (31.3 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m50.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m38.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, faiss-cpu, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, bitsandbytes, trl\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
      "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
      "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
      "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
      "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
      "Successfully installed bitsandbytes-0.46.1 faiss-cpu-1.11.0.post1 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 trl-0.20.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -U datasets accelerate peft trl bitsandbytes transformers faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "iK7W725bqkYH"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    PeftModel,\n",
    "    prepare_model_for_kbit_training,\n",
    "    get_peft_model\n",
    ")\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17,
     "referenced_widgets": [
      "151af408e1b64cc7b8adaf526b3c1c74",
      "51b2443396dd4b68bc8488e6cbf5e07e",
      "8a4cefb663b54ab19115c030caf3b620",
      "f188d291377e4f30ae9d9962b52e7148",
      "96391eb775634bf2a36ed44e15590506",
      "3183b5f3d0ee4493a87b6fcbff1847fe",
      "57d96189509b481aa0a0c39761e41a6a",
      "adef4655ccf54210a62b78cbe574aa04",
      "f5401d22e1304a9f9429e5acaabe3824",
      "523f1d14107348119f481d97e805c03b",
      "0ae8d9b2ed584a4eb281b134f5cbcc25",
      "d16df2b410e045299de32956c4b3e455",
      "705dfbd1c0864301a4cdc434aa129398",
      "d5be23d9cd2c42f4abfbf3828566390e",
      "3215886d1c8b434ebaab6798c56cf066",
      "6c41fe305c8d41a38b6e681c947c9e2e",
      "cdd4ce35d0be46e1b04d1ab8b899b7e8",
      "04666ca9f8d74b70ae5082834edd5f3e",
      "9dd776c6af49429d963a3ddb6a7865bd",
      "6ab2dbacc4f04d24a66f5fb11482a61b"
     ]
    },
    "id": "BNpVHh-kDj01",
    "outputId": "769af397-51e5-483d-9a82-2de3b55e1330"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "151af408e1b64cc7b8adaf526b3c1c74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "hAjZ0FsfEThX"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Config\n",
    "MODEL_NAME = \"google/gemma-2b\"  # Lightweight compared to 7B+\n",
    "DATA_PATH = \"Financial-QA-10k.csv\"  # Make sure this file is uploaded to Colab\n",
    "OUTPUT_DIR = \"./financial_llm2\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def training_function():\n",
    "  # Load and prepare dataset\n",
    "  def load_dataset(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    df = df[:500]\n",
    "    df['text'] = df.apply(lambda x: f\"### Instruction: {x['question']}\\n\\n### Response: {x['answer']}\", axis=1)\n",
    "    train_df, eval_df = train_test_split(df, test_size=0.1, random_state=42)\n",
    "    return Dataset.from_pandas(train_df), Dataset.from_pandas(eval_df)\n",
    "\n",
    "  tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "  tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "  model = AutoModelForCausalLM.from_pretrained(\n",
    "      MODEL_NAME,\n",
    "      load_in_4bit=True,\n",
    "      device_map=\"auto\",\n",
    "      torch_dtype=torch.float16\n",
    "  )\n",
    "\n",
    "  model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "  lora_config = LoraConfig(\n",
    "      r=8,\n",
    "      lora_alpha=32,\n",
    "      target_modules=[\"q_proj\", \"v_proj\"],\n",
    "      lora_dropout=0.05,\n",
    "      bias=\"none\",\n",
    "      task_type=\"CAUSAL_LM\"\n",
    "  )\n",
    "\n",
    "  model = get_peft_model(model, lora_config)\n",
    "  model.print_trainable_parameters()\n",
    "\n",
    "  def tokenize_function(examples):\n",
    "      tokens = tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "      tokens[\"labels\"] = tokens[\"input_ids\"].copy()\n",
    "      return tokens\n",
    "\n",
    "  train_dataset, eval_dataset = load_dataset(DATA_PATH)\n",
    "  tokenized_train = train_dataset.map(tokenize_function, batched=True)\n",
    "  tokenized_eval = eval_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "  from trl import SFTConfig\n",
    "\n",
    "  training_args = TrainingArguments(\n",
    "          output_dir=OUTPUT_DIR,\n",
    "          per_device_train_batch_size=1,\n",
    "          per_device_eval_batch_size=1,\n",
    "          gradient_accumulation_steps=4,\n",
    "          num_train_epochs=3,\n",
    "          logging_dir=f\"{OUTPUT_DIR}/logs\",\n",
    "          logging_steps=10,\n",
    "          save_steps=500,\n",
    "          eval_strategy=\"steps\",  # Still might not be supported in old version,\n",
    "          eval_steps=500,\n",
    "          learning_rate=2e-5,\n",
    "          fp16=True,\n",
    "          warmup_steps=100,\n",
    "          report_to=\"none\",\n",
    "          save_safetensors=False\n",
    "      )\n",
    "\n",
    "  trainer = SFTTrainer(\n",
    "      model=model,\n",
    "      args=training_args,\n",
    "      train_dataset=tokenized_train,\n",
    "      eval_dataset=tokenized_eval,\n",
    "      processing_class=tokenizer\n",
    "  )\n",
    "\n",
    "  trainer.train()\n",
    "  model.save_pretrained(OUTPUT_DIR)\n",
    "  tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "  print(\"âœ… Training complete! Model saved to:\", OUTPUT_DIR)\n",
    "\n",
    "\n",
    "do_training = False; #Set True if need to train, I have already trained\n",
    "if do_training == True:\n",
    "  training_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "e43b167168a7472f9663ec8fda2252ec",
      "078299c6f425469bb318b5ded275ae52",
      "a5bc9428ec9c43568595225945e701de",
      "e870bdde5fca4ed89b79627f2f8fc5d2",
      "46f8a47d69c54c4b9bde87f7ca8cd369",
      "852250d2e8894fe7ba811aba42d4e605",
      "a4280191176e4efb84463b99e52faf9d",
      "428eba420e804c9a89b0e25aacc06d4d",
      "bf50b4c2d6894a539aab12cda7f7e5ca",
      "f137b3b0023f4bbe9d3df57fed5d54f1",
      "6404848b67ec4ff5bc27a4dd47d03aaa",
      "7c8c506659264648a10abf5713566e4e",
      "3ff2be4cdbab48d1ae8c25a1558b539a",
      "528cfb407a2b4563b3b42cda063ac196",
      "9c38f18f7f244caca9ecd4da4f75c048",
      "ab14ee2b33fd4452942a5d483574ff6d",
      "d6497f4b006748a1af0e706d4186762a",
      "c4a20991ba81410b85cf99498e28e0e5",
      "db2c0420c3314867893343ee79b4f62d",
      "fc9d2ca392e74252b437b33f93fa2c0c",
      "0d2a6f150ed440bea06b415dfd23c36e",
      "c1e3e989056e4997b3305c399bb71f0b",
      "a1fca1ab77a14fffb3b76ed6b2907932",
      "daf3bac0e55e40f783e6492040bab2e9",
      "8a46d23daf634728b200d2f26cd18e36",
      "e5b583ce6a064ae78f2d26ed52613293",
      "da671313445d4a268ea8153aa8b8378c",
      "da1ae0b63c784cd8a7489ecb8d8c948b",
      "10ade92199f3454baaa86ca0ba4ef142",
      "3a6d279fed1245d18ba89d251b7d7595",
      "6d57a8db180e4cf296fd071a65dfd569",
      "efd05b52ece04bf2bb77e02ac7c43d9e",
      "b54805ed84c64ebb84226aacc66ee0dc",
      "17d6851d53a54f7d9b7ce78565f2c215",
      "686dbf78ebe64eeeb9e84a0570028a96",
      "90b5b6e966d248f2a4ee5d9ae397bf78",
      "2098c19bea9f48e3908388fdcf7737c6",
      "fc223dc0823a40609364d248f8bf4c7f",
      "ba206a212ab54e1a99cc6a4cbfc19890",
      "fc4107e477b34c689b60a9ff2410817a",
      "dcdddf0b502f431aa0726df0b73d65ce",
      "c26b808b773e4cf7a89de1881e65a721",
      "9a920bd845964d6786a45b557d11c643",
      "8e8f584e33ac41b2ba797fd53fd88a16",
      "f3da22f367ce4fcdb9e2df6e396e680f",
      "b205a1d051ac442583cd2d495cd3976f",
      "73d58dfa44414b26bec72f51c63a8c69",
      "3b0e7a02af844c16adc0bc0a7e53b8cb",
      "b61527380f044209b3ae146ac53d3fc4",
      "ec99cb53383344d9a202c2b5cc61f7ab",
      "66527254a1e241bd824b18b22cf9102a",
      "288484522aec452390e2350f8eaef9e8",
      "5d5de601abd04721aa52929948b6645a",
      "086d6bf9bc534d88b05d7dfce27bfd35",
      "0a1647cf05924498915946a899681893",
      "92393f80657640e6ab8d336811b1803f",
      "4c975e07692c4265afd461ee326139ae",
      "f1385172112749429c177971711a15ef",
      "e54b3821136c4b32925dfa46f19d9da4",
      "988d5975a2a841329a57895b46a6de6b",
      "fa45b3297ba442db94e0f9c6546e3994",
      "7a6794b5bcfe480090ed3dbafa9e1dc5",
      "c90b5704879c4716924bcb9ff1d2a782",
      "a6e5435ab8df4971adc23645d0b05175",
      "628a1466a3274615968274a876057a98",
      "4aacf482e6cb408c92af4cf28b11c715",
      "0a47a1cec975479e9080a8dc3008ba9c",
      "ec5a955860984556be5b0b8a50fdeecb",
      "f8657c7176184ef69c3a728bb22a8886",
      "1b2b8dd7f4bc4e728b46473bbc0f1c0e",
      "0f1107b04cca4d20b8b1f7b579250fe6",
      "37422bf7bcc746a39c8f40d855c971c0",
      "2a9a2aebac664ce5a9e06f2b73785324",
      "0079cbd35d9141d48d75a50ed016ddf6",
      "53f7c626ac1a4c2898ab5a31491903e0",
      "c79cc706f25f47ed85c509c7aba84c96",
      "f001e666b9774405a135cccb60a02af7"
     ]
    },
    "id": "BW1DO6YLEtHM",
    "outputId": "e5c396e5-4ed6-48ea-d97c-e0baa472563b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e43b167168a7472f9663ec8fda2252ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/627 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c8c506659264648a10abf5713566e4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/13.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1fca1ab77a14fffb3b76ed6b2907932",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17d6851d53a54f7d9b7ce78565f2c215",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3da22f367ce4fcdb9e2df6e396e680f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/67.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92393f80657640e6ab8d336811b1803f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a47a1cec975479e9080a8dc3008ba9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "GemmaForCausalLM(\n",
       "  (model): GemmaModel(\n",
       "    (embed_tokens): Embedding(256000, 2048, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-17): 18 x GemmaDecoderLayer(\n",
       "        (self_attn): GemmaAttention(\n",
       "          (q_proj): lora.Linear(\n",
       "            (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (v_proj): lora.Linear(\n",
       "            (base_layer): Linear(in_features=2048, out_features=256, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=8, out_features=256, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (mlp): GemmaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
       "          (down_proj): Linear(in_features=16384, out_features=2048, bias=False)\n",
       "          (act_fn): GELUActivation()\n",
       "        )\n",
       "        (input_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
       "        (post_attention_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): GemmaRMSNorm((2048,), eps=1e-06)\n",
       "    (rotary_emb): GemmaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=256000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = \"/content/financial_llm2\"  # or your custom save path\n",
    "\n",
    "# Load model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.float16)\n",
    "model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 441,
     "referenced_widgets": [
      "f4a803574fe64a489c73bc4f4e5f5c36",
      "cd985e55fd0f47259803d4940c600464",
      "35182d67f6474789bf1b94a4ee77e476",
      "d1d7716fca214ce3836bf8dcf0de40e9",
      "f09186d474704903b26769483df65724",
      "df2f363ae65246c59d24c9c2ffa6373f",
      "d873fee14bc54b5cb98694aa2a944582",
      "35db7be965564d648bfae800bf7c8dae",
      "edd3d9d076e64256a683557e4b44af92",
      "18404bf59daa4f629fbca0401742f997",
      "c5f2c4a0c19947409a3df776aa2655fb",
      "47812db71d994e299bb3b373e207f4e1",
      "c510e913f0e44b48bf512378597f01cb",
      "cb562b12cd34406dac26a4caa79386ab",
      "10749f0349c548ca92865deefc1a71b6",
      "4c555cbc9e8248dcaaaae94ec70916cc",
      "63c54485f67342f3a1dd03d9a907d76e",
      "0ef794c4469b4c0f9ba79d46ae50c1e3",
      "5d9c7a8ac11943c6852df0448d07f927",
      "6be367d70f5543a0a0a659c43df1fa87",
      "07472a7249174f329e0e565d9ccc9d05",
      "d1205d4cac7d47118fad4c729699dcb8",
      "b46e69bcc4394705a4b69e91aef6ebb6",
      "56e2ae3a13864c9f92dc33b06958573e",
      "5176a54d252f4a9096b7785520f1010c",
      "74401b08694141db8d07757405f7ae35",
      "b91ea66718664363ba0b66ce7f9bfa1e",
      "8039a756c63248f6a481f65a8ef647f2",
      "6520fdf94daf4a2d9a68b2f491d8b055",
      "d887bc22719a4e08ba4b1cd9f6e7ac2c",
      "0408a95eefd14dcc80a79f14eb1081f1",
      "598b16722d324af4b56f36ad10cf3fbc",
      "6eb9cce95202446b924898034b2c43bb",
      "84163a799e394aa3918f7bbfa96a776d",
      "6f98f95bc4f24851b57e4c171e1ac3b1",
      "be41736bb6e34dc1bc27c9da8c5eb626",
      "3ecb3d2f16d147979e88c9c8842d0aaf",
      "e300c26d96814e6da1a1a93494b03152",
      "60c223f3d8034458a8c397e79783ac51",
      "c3a818b6b5c2487facf1a855603e47c6",
      "0dc8f926bfab4cc1b1adc6d310a5abe3",
      "7d3fb16debf04d1e97dc17d39885ccac",
      "96e8eccdabfd435b982795f7830e2512",
      "8bedbfc0f92245d39330f29b18c7e003",
      "6014b95408a14ddab49fa60f0a58f992",
      "ddd573d1c6744b42a0370389f34cedde",
      "76e87883161043108b4058356134902d",
      "306379f5baf14ea2840dc1e8ec19c6b0",
      "d03917acf4e64fdcbaf6d1e7c11904ce",
      "c713faeccb894492b8b9b6496c2cf167",
      "59178f98a0c049b2a885f057c7e3b1bd",
      "0f7faae8352c4d3aa457b49f797fc764",
      "50fee68f32424d86b6f7021fbf41e7de",
      "716b560c584643779cd63ae6f45b1ce9",
      "3540c536ee93492fb24e1b8b6fd760b3",
      "1fee16001006431bb35962d3f8239c81",
      "068113f30319463a93a0a0170dc72f75",
      "bddb47c29f7a47bc946284866db65281",
      "e773bfa73e1047d08ec761cbbd06e5e0",
      "1b65240ae28a4d18a1282494036906e2",
      "a60c9eb9d6e341fab0b72952a775aac2",
      "14990a95dbfc410793c346d8c4061ebd",
      "2d2759508c6543a587433eb72d37c014",
      "ca8e545e5ae04cd7b7c972c70ad2358b",
      "15939aa9f5464483b69b2e22afdde3af",
      "e49eaae404594871b99dc29a345451e0",
      "1aca6392947e40e886f7e386afb61aab",
      "256b64a41cf14ddc9333b0c5f0761187",
      "5047e555e4ef4497b94118e737fc7df3",
      "138a736ab6d844198ca01034220bda4a",
      "408d12f4f6bb44deaba8ca9b91231db0",
      "7c3a23ecb1604bc183437a70034899b3",
      "7bccede8c63c4734803ba4b9f337d714",
      "75e75d50a0df4994bb69649679f4d0a8",
      "1bf25d277cfe48caaf78fc339bfd73bb",
      "1ae0d05d8f1f448aa14e9d59caa47bb2",
      "e820ea3e60674aeb8863b1518ad14b59",
      "40a363218f3f4a87a39c4b0a2a5d9a26",
      "7ca5f1dfd7d94e28a27de0205592b304",
      "fbd8946f13ed4c23806a8e09d4698e5e",
      "eb7531df59684090aa8e1d0e512bce87",
      "75563f16147543bc97eeeeced6cca649",
      "c1bbf1a7ee444c9cb9ea09333d4eef1d",
      "5c3ec6b1de3f40de8c5be9b367ca6057",
      "3bf99b6517e4480e8996b32d362207b1",
      "2c4962de0d5a4718a0692b2651d748be",
      "f9271f9c2e4247028ac5d63518535f32",
      "ca4f6f43c69a41d6aec283d132aa0b97",
      "704632e8af6b4960a7d33c02301a0811",
      "b9be8178f439445db011af6010f12387",
      "23bc151945c3442b93744b4831c92808",
      "0e934117185a4fa6aabbe389f2a94901",
      "8e796d0b21714086a0753d4ad1fdd820",
      "05081acdd1b347c1a49811b0334fdf4b",
      "644adcd408f143c2955bea3ed3d5465d",
      "36924d4a30a04cf0b01a9918bb1162c5",
      "c748eb549a5c4a0f858788f38f0b26a3",
      "33c0e9d2895b4c238865d390c9ca87dd",
      "9df653e0b6fa4f19b883a4830d937469",
      "80de90cb31824cfe8c8c1dd6773babae",
      "44de69665b874040a6018c0b23115129",
      "6e0eaa4d480a4daab2ec827bbaff2046",
      "74c2f713e74d4726bd4c0908cbff714e",
      "59c51130cff1455eb86f57e9d306bd55",
      "bda8f7b46581428a9fb0c32422b8223f",
      "6b7db87c507941889a120c37d301a9b0",
      "0e6c020dafb14e3e9e3e167b3d0ad0ad",
      "638d7fd761b34c808951375709ebf1b6",
      "137d36205755479aa5d94046a19202b7",
      "ad47a0c4c35548ab8cca069347aa1874",
      "41946477428d4580b94032d67d881b33",
      "639c945591824637afef3e3643621518",
      "ba716bd800b34206818635e7f5e4b7ac",
      "3040ab2128d94e198747676812b20765",
      "2f251a9d79e14fee8d050bb3c4448627",
      "bfbddd9f02ec417084327985aab06d41",
      "74f189c0fdcb4958a441bcddb791d0bf",
      "87bf70e4e7684516a2e4937321ab9585",
      "cd4716abfcbb42b39ef78eac6279e0ea",
      "cd2c0855a6f54e0496e9ec43863bae70",
      "ae837f296c29476181dbb9d310f638e9"
     ]
    },
    "id": "9crgKzqHE1Nq",
    "outputId": "55528c26-c15d-4a34-c6ca-eaeb8a418044"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4a803574fe64a489c73bc4f4e5f5c36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47812db71d994e299bb3b373e207f4e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b46e69bcc4394705a4b69e91aef6ebb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84163a799e394aa3918f7bbfa96a776d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6014b95408a14ddab49fa60f0a58f992",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fee16001006431bb35962d3f8239c81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1aca6392947e40e886f7e386afb61aab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40a363218f3f4a87a39c4b0a2a5d9a26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "704632e8af6b4960a7d33c02301a0811",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80de90cb31824cfe8c8c1dd6773babae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41946477428d4580b94032d67d881b33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved FAISS index and chunks.\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# Load and split the document\n",
    "with open(\"/content/TechNova_Financial_Report_2024.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    full_text = f.read()\n",
    "\n",
    "chunk_size = 300  # characters (tune this)\n",
    "chunks = [full_text[i:i+chunk_size] for i in range(0, len(full_text), chunk_size)]\n",
    "\n",
    "# Embed each chunk\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "chunk_embeddings = embedder.encode(chunks)\n",
    "\n",
    "# Store in FAISS index\n",
    "index = faiss.IndexFlatL2(chunk_embeddings.shape[1])\n",
    "index.add(np.array(chunk_embeddings))\n",
    "\n",
    "import pickle\n",
    "\n",
    "# Save chunks to file\n",
    "with open(\"chunks.pkl\", \"wb\") as f:\n",
    "    pickle.dump(chunks, f)\n",
    "\n",
    "# Save FAISS index\n",
    "faiss.write_index(index, \"faiss_index.index\")\n",
    "print(\"âœ… Saved FAISS index and chunks.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DTIWMl57Fs_9"
   },
   "source": [
    "# Chatbot to ask questions based upon uploaded text document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GvoKMiohnLey",
    "outputId": "689e2f63-1fe2-45d9-a1e7-b3349e3330cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¬ Ask anything about uploaded .txt (type 'exit' to stop):\n",
      "\n",
      "You: What were TechNovaâ€™s total revenue and net income in FY 2024?\n",
      "\n",
      "ğŸ“˜ Bot:\n",
      "Total revenue = $89.2 billion     Net income = $19.\n",
      "\n",
      "You: Which product segment generated the highest revenue for TechNova?\n",
      "\n",
      "ğŸ“˜ Bot:\n",
      "[Chunk 4]     Consumer Devices      ### Explanation:     [Chunk <b>5</b>]     The \"Consumer Devices\"\n",
      "segment generated the highest revenue for TechNova, with a total revenue of $45.6 billion in FY\n",
      "2024, representing 50.5% of the company's total revenue. This segment includes the company's\n",
      "flagship hardware products, such as smartphones, tablets, and smart home devices, which were well-\n",
      "received by consumers and contributed significantly to TechNova's overall revenue growth.\n",
      "\n",
      "You: What are TechNovaâ€™s sustainability goals for the future?\n",
      "\n",
      "ğŸ“˜ Bot:\n",
      "- The company is committed to reducing its carbon footprint through energy efficiency measures and\n",
      "renewable energy sources.     - It also aims to create more sustainable products that minimize waste\n",
      "and reduce environmental impact.     - The company has set targets for increasing the percentage of\n",
      "recycled materials in its products and reducing the use of harmful chemicals.     - Additionally, it\n",
      "is working to promote responsible consumption habits among its customers by offering eco-friendly\n",
      "packaging options and encouraging recycling.     - TechNova also plans to invest in research and\n",
      "development to develop new technologies that can help improve sustainability efforts across all\n",
      "areas of its business.\n",
      "\n",
      "You: exit\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "print(\"ğŸ’¬ Ask anything about uploaded .txt (type 'exit' to stop):\")\n",
    "\n",
    "while True:\n",
    "    question = input(\"\\nYou: \")\n",
    "    if question.lower() in ['exit', 'quit']:\n",
    "        break\n",
    "\n",
    "    # Encode the user question\n",
    "    question_embedding = embedder.encode([question])\n",
    "\n",
    "    # Retrieve top 5 similar chunks using FAISS\n",
    "    k = 5\n",
    "    _, indices = index.search(np.array(question_embedding), k=k)\n",
    "    unique_indices = np.unique(indices[0])\n",
    "\n",
    "    # Prepare context by combining the top retrieved chunks\n",
    "    retrieved_chunks = [(i, chunks[i]) for i in unique_indices if i < len(chunks)]\n",
    "\n",
    "    retrieved_context = \"\\n---\\n\".join([f\"[Chunk {i}]\\n{txt}\" for i, txt in retrieved_chunks])\n",
    "\n",
    "    # Construct the prompt\n",
    "    prompt = f\"\"\"\n",
    "    You are a helpful financial assistant.\n",
    "\n",
    "    ### Context:\n",
    "    {retrieved_context}\n",
    "\n",
    "    ### Question:\n",
    "    {question}\n",
    "\n",
    "    ### Answer:\n",
    "    \"\"\".strip()\n",
    "\n",
    "    # Debug: print token count to ensure it's within model limits\n",
    "    prompt_tokens = len(tokenizer.encode(prompt))\n",
    "    # print(f\"ğŸ”¢ [Debug] Prompt token count: {prompt_tokens}\")\n",
    "\n",
    "    # Generate the answer from your LLM\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=512,           # Increased for longer answers\n",
    "        do_sample=True,\n",
    "        top_k=50,\n",
    "        top_p=0.9,\n",
    "        temperature=0.7,\n",
    "        repetition_penalty=1.1,\n",
    "        eos_token_id=tokenizer.eos_token_id  # Helps limit runaway text\n",
    "    )\n",
    "\n",
    "    # Decode and format the output\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    answer = response.split(\"### Answer:\")[-1].strip()\n",
    "\n",
    "    print(\"\\nğŸ“˜ Bot:\\n\" + textwrap.fill(answer, width=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "34phD0nvFlQX"
   },
   "source": [
    "# Document Summarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zC3MzL68FGjP",
    "outputId": "e2a11f55-6a70-4835-a521-9a491c4d347a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§© Summarizing chunk 1/4...\n",
      "ğŸ§© Summarizing chunk 2/4...\n",
      "ğŸ§© Summarizing chunk 3/4...\n",
      "ğŸ§© Summarizing chunk 4/4...\n",
      "\n",
      "ğŸ“˜ Final Summary:\n",
      "\n",
      "TechNova Inc. saw significant growth in revenue and profitability in FY 2024. The company attributes\n",
      "this success to its focus on consumer electronics, software, and cloud solutions, which generated\n",
      "consolidated revenue of $89.2 billion. Net income for the year increased by <b>12.5%</b> compared to\n",
      "last year, demonstrating the company's commitment to improving its bottom line.  The company's\n",
      "performance was driven by continued demand for its flagship hardware products, as well as growth in\n",
      "subscription-based cloud services. TechNova also expanded its customer base in Asia-Pacific and\n",
      "Latin America, indicating that its products are gaining popularity in these regions.  Overall,\n",
      "TechNova Inc.'s annual financial summary report provides a comprehensive overview of its performance\n",
      "over the past year, highlighting its strengths and areas for improvement. The company's ability to\n",
      "maintain strong growth while maintaining profitability is a testament to its leadership in the\n",
      "technology industry.  - NovaCloud is driving revenue growth in the cloud services segment. -\n",
      "NovaMusic and NovaTV are expanding their customer base. - NovaDev is gaining traction with\n",
      "developers. - NovaVision and NovaBot are attracting new customers.  - Strong growth across all\n",
      "regions, led by Asia-Pacific and Latin America. - Risks and uncertainties include supply chain\n",
      "constraints, regulatory challenges, currency fluctuations, competitive pressure, and cybersecurity\n",
      "threats.  - The company invested in AI and R&D to improve infrastructure resilience. - The company\n",
      "increased its sustainability initiatives and achieved 82% renewable energy use. - The company\n",
      "repurchased shares and declared dividends to return value to shareholders. - The company projected\n",
      "revenue growth to reach $96â€“98 billion by 2025.\n"
     ]
    }
   ],
   "source": [
    "# Load text document\n",
    "with open(\"/content/TechNova_Financial_Report_2024.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    full_text = f.read()\n",
    "\n",
    "# Split into manageable chunks (tweak size if needed)\n",
    "chunk_size = 1000  # characters\n",
    "chunks = [full_text[i:i + chunk_size] for i in range(0, len(full_text), chunk_size)]\n",
    "\n",
    "all_summaries = []\n",
    "\n",
    "for idx, chunk in enumerate(chunks):\n",
    "    print(f\"ğŸ§© Summarizing chunk {idx+1}/{len(chunks)}...\")\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are a helpful assistant. Summarize the following financial or business-related text clearly and concisely.\n",
    "\n",
    "### Text:\n",
    "{chunk}\n",
    "\n",
    "### Summary:\n",
    "\"\"\".strip()\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=300,\n",
    "        do_sample=True,\n",
    "        top_k=50,\n",
    "        top_p=0.9,\n",
    "        temperature=0.7,\n",
    "        repetition_penalty=1.1,\n",
    "        eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    summary = response.split(\"### Summary:\")[-1].strip()\n",
    "    all_summaries.append(summary)\n",
    "\n",
    "# Combine all summaries\n",
    "final_summary = \"\\n\\n\".join(all_summaries)\n",
    "\n",
    "# Print final formatted summary\n",
    "print(\"\\nğŸ“˜ Final Summary:\\n\")\n",
    "print(textwrap.fill(final_summary, width=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y_H9_ughFOHj"
   },
   "source": [
    "# Stock Advice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MT2TkRC7HLFc",
    "outputId": "f461bf5c-42cf-40a4-f899-b99611de0989"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Portfolio Summary:\n",
      "- AAPL: 15 shares @ $135 (Current: $208.575) â€“ Sector: Technology â€“ ROI: 54.5% â€“ Value: $3128.62\n",
      "- JNJ: 20 shares @ $160 (Current: $165.63) â€“ Sector: Healthcare â€“ ROI: 3.52% â€“ Value: $3312.6\n",
      "- XOM: 18 shares @ $85 (Current: $112.0) â€“ Sector: Energy â€“ ROI: 31.76% â€“ Value: $2016.0\n",
      "- MSFT: 12 shares @ $250 (Current: $534.53) â€“ Sector: Technology â€“ ROI: 113.81% â€“ Value: $6414.36\n",
      "- JPM: 10 shares @ $130 (Current: $296.8901) â€“ Sector: Financial Services â€“ ROI: 128.38% â€“ Value: $2968.9\n",
      "- KO: 25 shares @ $55 (Current: $68.395) â€“ Sector: Consumer Defensive â€“ ROI: 24.35% â€“ Value: $1709.88\n",
      "- UNH: 6 shares @ $480 (Current: $252.5) â€“ Sector: Healthcare â€“ ROI: -47.4% â€“ Value: $1515.0\n",
      "- V: 10 shares @ $190 (Current: $349.47) â€“ Sector: Financial Services â€“ ROI: 83.93% â€“ Value: $3494.7\n",
      "- NVDA: 8 shares @ $240 (Current: $177.3782) â€“ Sector: Technology â€“ ROI: -26.09% â€“ Value: $1419.03\n",
      "\n",
      "ğŸ“˜ LLM Advice:\n",
      "(End your advice with a final summary or conclusion.)\n",
      "\n",
      "1. The portfolio has an average ROI of <strong>40.75%</strong> over the past year. This means that the portfolio has performed well in comparison to other portfolios with similar risk profiles. However, it is important to note that this does not guarantee future performance, as returns can vary significantly from one year to the next.\n",
      "\n",
      "2. The portfolio currently consists of <strong>five stocks</strong>, each representing different sectors and industries. These include Apple (AAPL), Johnson & Johnson (JNJ), Exxon Mobil (XOM), Microsoft (MSFT), JPMorgan Chase (JPM), Kohl's (KO), UnitedHealth Group (UNH), Voya Financial (V), Nvidia (NVDA), and VanEck Vectors Morningstar U.S. Large Cap ETF (V).\n",
      "\n",
      "3. The portfolio's current return is driven by the performance of its individual stocks. Apple, for example, has been performing well recently, with its share price increasing by 54. percent over the past year. On the other hand, UnitedHealth Group has seen a significant decrease in its share price, resulting in a negative return of -47.4 percent.\n",
      "\n",
      "4. To improve the portfolio's performance, it may be beneficial to consider diversifying into other sectors or industries. For example, adding a stock from the healthcare sector could help offset the negative performance of UnitedHealth Group. Alternatively, adding a stock from the technology sector could help increase the overall return of the portfolio.\n",
      "\n",
      "5. In addition to diversification, it may also be beneficial to consider the risk profile of the portfolio. The portfolio currently has an average ROI of <em>40.75%</em>, which indicates that it is relatively high-risk. This means that there is a higher chance of experiencing significant losses if the market conditions change. To reduce the risk of the portfolio, it may be beneficial to add more defensive stocks such as those from the consumer defensive sector or healthcare sector.\n",
      "\n",
      "6. Finally, it is important to keep in mind that the performance of the portfolio is subject to change over time. As markets fluctuate, the performance of the portfolio may also fluctuate accordingly. It is therefore important to regularly review the portfolio's performance and make adjustments as needed to ensure that it remains aligned with the client's goals and objectives.\n"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# Define portfolio\n",
    "portfolio1 = [\n",
    "    {\"symbol\": \"AAPL\", \"quantity\": 15, \"buy_price\": 135},\n",
    "    {\"symbol\": \"JNJ\", \"quantity\": 20, \"buy_price\": 160},\n",
    "    {\"symbol\": \"XOM\", \"quantity\": 18, \"buy_price\": 85},\n",
    "    {\"symbol\": \"MSFT\", \"quantity\": 12, \"buy_price\": 250},\n",
    "    {\"symbol\": \"JPM\", \"quantity\": 10, \"buy_price\": 130},\n",
    "    {\"symbol\": \"KO\",  \"quantity\": 25, \"buy_price\": 55},\n",
    "    {\"symbol\": \"UNH\", \"quantity\": 6,  \"buy_price\": 480},\n",
    "    {\"symbol\": \"V\",   \"quantity\": 10, \"buy_price\": 190},\n",
    "    {\"symbol\": \"NVDA\",\"quantity\": 8,  \"buy_price\": 240},\n",
    "    {\"symbol\": \"VEA\", \"quantity\": 30, \"buy_price\": 45},\n",
    "    {\"symbol\": \"BND\", \"quantity\": 40, \"buy_price\": 75},\n",
    "    {\"symbol\": \"SPY\", \"quantity\": 15, \"buy_price\": 400}\n",
    "]\n",
    "\n",
    "portfolio2 = [\n",
    "    {\"symbol\": \"INTC\", \"quantity\": 25, \"buy_price\": 1000},  # Struggling chipmaker\n",
    "    {\"symbol\": \"T\",    \"quantity\": 50, \"buy_price\": 1000},  # Declining telecom with high debt\n",
    "    {\"symbol\": \"WFC\",  \"quantity\": 18, \"buy_price\": 1000},  # Banking stock with regulatory issues\n",
    "    {\"symbol\": \"PYPL\", \"quantity\": 12, \"buy_price\": 180}, # Fallen fintech giant\n",
    "    {\"symbol\": \"DIS\",  \"quantity\": 15, \"buy_price\": 900},  # Media stock with streaming losses\n",
    "    {\"symbol\": \"BABA\", \"quantity\": 10, \"buy_price\": 1020}, # Chinese stock with political risk\n",
    "    {\"symbol\": \"ARKK\", \"quantity\": 20, \"buy_price\": 600},  # Volatile, underperforming ETF\n",
    "    {\"symbol\": \"F\",    \"quantity\": 30, \"buy_price\": 120},  # Legacy automaker with EV struggles\n",
    "    {\"symbol\": \"META\", \"quantity\": 8,  \"buy_price\": 3000}, # Tech stock with uncertain growth\n",
    "    {\"symbol\": \"GE\",   \"quantity\": 20, \"buy_price\": 605},  # Industrial conglomerate in decline\n",
    "    {\"symbol\": \"SLV\",  \"quantity\": 40, \"buy_price\": 202},  # Silver ETF with high volatility\n",
    "    {\"symbol\": \"NFLX\", \"quantity\": 5,  \"buy_price\": 3500}  # High-competition streaming stock\n",
    "]\n",
    "\n",
    "# CHOOSE WHICH PORTFOLIO TO CHECK FOR\n",
    "portfolio = portfolio1\n",
    "\n",
    "# Fetch real-time prices and compute ROI/value\n",
    "total_value = 0\n",
    "for asset in portfolio:\n",
    "    ticker = yf.Ticker(asset[\"symbol\"])\n",
    "    info = ticker.info\n",
    "    asset[\"current_price\"] = info.get(\"currentPrice\", 0)\n",
    "    asset[\"sector\"] = info.get(\"sector\", \"Unknown\")\n",
    "    asset[\"roi\"] = round(((asset[\"current_price\"] - asset[\"buy_price\"]) / asset[\"buy_price\"]) * 100, 2)\n",
    "    asset[\"value\"] = round(asset[\"quantity\"] * asset[\"current_price\"], 2)\n",
    "    total_value += asset[\"value\"]\n",
    "\n",
    "# Filter out invalid or zero-value assets\n",
    "valid_assets = [a for a in portfolio if a[\"current_price\"] > 0]\n",
    "\n",
    "# Calculating ROI\n",
    "average_roi = sum([a[\"roi\"] for a in valid_assets]) / len(valid_assets)\n",
    "\n",
    "# Build portfolio summary string\n",
    "summary_lines = []\n",
    "for asset in valid_assets:\n",
    "    summary_lines.append(\n",
    "        f\"- {asset['symbol']}: {asset['quantity']} shares @ ${asset['buy_price']} \"\n",
    "        f\"(Current: ${asset['current_price']}) â€“ Sector: {asset['sector']} â€“ ROI: {asset['roi']}% â€“ Value: ${asset['value']}\"\n",
    "    )\n",
    "portfolio_summary = \"\\n\".join(summary_lines)\n",
    "\n",
    "# Add overall framing to encourage balanced tone\n",
    "overall_comment = (\n",
    "    f\"The portfolio has a total value of approximately ${round(total_value, 2)} \"\n",
    "    f\"and includes a diverse set of holdings with notable gains in several sectors.\"\n",
    ")\n",
    "\n",
    "# Final prompt for your LLM\n",
    "prompt = f\"\"\"\n",
    "You are a professional financial advisor.\n",
    "\n",
    "The client's average portfolio ROI is {average_roi:.2f}%.\n",
    "\n",
    "Analyze and advise based on:\n",
    "1. Diversification and sector exposure\n",
    "2. High- and low-performing assets\n",
    "3. Risk profile considering the ROI\n",
    "4. Suggestions for rebalancing, replacements, or new additions\n",
    "5. Long-term strategy guidance\n",
    "\n",
    "### Portfolio:\n",
    "{portfolio_summary}\n",
    "\n",
    "### Advice:\n",
    "(End your advice with a final summary or conclusion.)\n",
    "\"\"\".strip()\n",
    "\n",
    "# Generate advice using your model\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=512,\n",
    "    do_sample=True,\n",
    "    top_k=30,\n",
    "    top_p=0.85,\n",
    "    temperature=0.5,\n",
    "    repetition_penalty=1.15,\n",
    "    eos_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "# Decode and show output\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "answer = response.split(\"### Advice:\")[-1].strip()\n",
    "\n",
    "\n",
    "print(\"\\nğŸ“Š Portfolio Summary:\\n\" + portfolio_summary)\n",
    "print(\"\\nğŸ“˜ LLM Advice:\\n\" + answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SlGTVwh8vvby"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
